{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Spark - run SQL queries\n\n* ***\uc6d0\ub798 \uc18c\uc2a4 \uac00\uc9c0\uace0 \uc548\ub3cc\uc544\uac00\ub124\uc694. \uc800\uc790\uac00 testing \ud558\uace0 Community \uc5d0 \uc62c\ub838\uc744\ub54c\uc640 \ub2ec\ub9ac \uac1c\uc778 \ud658\uacbd\uc5d0 \ubcc0\ud654\uac00 \uc788\uc5c8\uc744 \uac70\ub77c\uace0 \uc0dd\uac01\ub429\ub2c8\ub2e4.***\n* ***pandas\ub85c box \uc5d0 \uc788\ub294 \ud654\uc77c\uc744 \uc77d\uc5b4\uc11c, Spark SQL\ub85c  query\ub97c \ub3cc\ub824\ubd05\ub2c8\ub2e4.***\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of contents\nThis notebook contains these main sections:\n\n1. [Load a DataFrame](#Load_a_DataFrame)\n2. [Create an SQLContext](#Create_an_SQLContext)\n3. [Create a Spark DataFrame](#Create_a_Spark_DataFrame)\n4. [Aggregate data after grouping by columns](#Aggregate_data_after_grouping_by_columns)\n5. [Operate on columns](#Operate_on_columns)\n6. [Run SQL queries from the Spark DataFrame](#Run_SQL_queries_from_the_Spark_DataFrame)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting pyspark\n  Downloading https://files.pythonhosted.org/packages/58/49/45370cc153a6adcf2c304a3c06e801ed3c9650d0f852e7fde04bd8ffb534/pyspark-2.3.0.tar.gz (211.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 211.9MB 4.2kB/s eta 0:00:01 2% |\u2588                               | 6.3MB 46.4MB/s eta 0:00:05    56% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 118.7MB 50.6MB/s eta 0:00:0201\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a      | 170.6MB 47.2MB/s eta 0:00:01\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f     | 173.0MB 48.4MB/s eta 0:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b     | 176.1MB 47.3MB/s eta 0:00:01\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588     | 178.3MB 49.7MB/s eta 0:00:01    | 183.8MB 48.0MB/s eta 0:00:010:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 188.0MB 47.1MB/s eta 0:00:01\ufffd\u2588\u2588\u258a   | 190.0MB 46.9MB/s eta 0:00:01\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 191.9MB 48.6MB/s eta 0:00:01B/s eta 0:00:01.0MB 49.9MB/s eta 0:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 198.0MB 47.5MB/s eta 0:00:01.2MB/s eta 0:00:01 95% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 201.7MB 50.6MB/s eta 0:00:011\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 205.8MB 49.7MB/s eta 0:00:01\ufffd| 208.2MB 53.2MB/s eta 0:00:011\n\u001b[?25hCollecting py4j==0.10.6 (from pyspark)\n  Downloading https://files.pythonhosted.org/packages/4a/08/162710786239aa72bd72bb46c64f2b02f54250412ba928cb373b30699139/py4j-0.10.6-py2.py3-none-any.whl (189kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 194kB 4.9MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/d9/db/ff/e6f3a8a564163ea64bc2072357e77b3404d10f91be48352796\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.6 pyspark-2.3.0\n"
                }
            ], 
            "source": "!pip install pyspark"
        }, 
        {
            "source": "<a id='Load_a_DataFrame'></a>\n## 1. Load a DataFrame\nA DataFrame is a distributed collection of data that is organized into named columns. The Python pandas DataFrame that you will create will load an external DataFrame called **mtcars**, which includes observations on the following 11 variables:\n\n`[, 1]\tmpg     Miles / (US) gallon`  \n`[, 2]\tcyl     Number of cylinders`  \n`[, 3]\tdisp\tDisplacement (cu. in.)`  \n`[, 4]\thp      Gross horsepower`  \n`[, 5]\tdrat    Rear axle ratio`  \n`[, 6]\twt      Weight (1000 lbs)`  \n`[, 7]\tqsec    1/4 mile time (seconds)`  \n`[, 8]\tvs      0 = V-engine, 1 = straight engine`  \n`[, 9]\tam      Transmission (0 = automatic, 1 = manual)`  \n`[,10]\tgear    Number of forward gears`  \n`[,11]\tcarb    Number of carburetors`\n\n\n* ***library \ucd94\uac00 \ud588\uc2b5\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nimport pyspark.sql "
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd\nmtcars = pd.read_csv('https://ibm.box.com/shared/static/f1dhhjnzjwxmy2c1ys2whvrgz05d1pui.csv')"
        }, 
        {
            "source": "Preview the first 3 rows of the DataFrame by using the `head()` method:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>car</th>\n      <th>mpg</th>\n      <th>cyl</th>\n      <th>disp</th>\n      <th>hp</th>\n      <th>drat</th>\n      <th>wt</th>\n      <th>qsec</th>\n      <th>vs</th>\n      <th>am</th>\n      <th>gear</th>\n      <th>carb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mazda RX4</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160.0</td>\n      <td>110</td>\n      <td>3.90</td>\n      <td>2.620</td>\n      <td>16.46</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mazda RX4 Wag</td>\n      <td>21.0</td>\n      <td>6</td>\n      <td>160.0</td>\n      <td>110</td>\n      <td>3.90</td>\n      <td>2.875</td>\n      <td>17.02</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Datsun 710</td>\n      <td>22.8</td>\n      <td>4</td>\n      <td>108.0</td>\n      <td>93</td>\n      <td>3.85</td>\n      <td>2.320</td>\n      <td>18.61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "             car   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n0      Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n1  Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n2     Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n\n   carb  \n0     4  \n1     4  \n2     1  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "mtcars.head(3)"
        }, 
        {
            "source": "<a id='Create_an_SQLContext'></a>\n## 2. Create an SQLContext\nTo work with a DataFrame, you need an SQLContext class object, and to create a basic one, all you need is a SparkContext. A SparkContext represents the connection to a Spark cluster, and a SparkContext class object named sc, which has been created for you, is used to initialize the SQLContext:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "* ***library import \ud558\uace0 \ub098\uc11c\ub3c4 sc \ub54c\ubb38\uc5d0 \uc5d0\ub7ec\ub098\uc11c \ucf54\ub4dc 2\uc904 \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "conf = SparkConf().setAppName(\"Spark SQL\")\nsc = SparkContext(conf=conf)\nsqlContext = SQLContext(sc)"
        }, 
        {
            "source": "<a id='Create_a_Spark_DataFrame'></a>\n## 3. Create a Spark DataFrame\nUsing the SQLContext class object and the loaded local DataFrame, create a Spark DataFrame and print the schema, or structure, of the DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- car: string (nullable = true)\n |-- mpg: double (nullable = true)\n |-- cyl: long (nullable = true)\n |-- disp: double (nullable = true)\n |-- hp: long (nullable = true)\n |-- drat: double (nullable = true)\n |-- wt: double (nullable = true)\n |-- qsec: double (nullable = true)\n |-- vs: long (nullable = true)\n |-- am: long (nullable = true)\n |-- gear: long (nullable = true)\n |-- carb: long (nullable = true)\n\n"
                }
            ], 
            "source": "sdf = sqlContext.createDataFrame(mtcars) \nsdf.printSchema()"
        }, 
        {
            "source": "* ***Spark DataFrame \uc5d0 \ub123\uc624 \ub193\uace0 10 row \ub9cc \ucd9c\ub825\ud574\ubd05\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|              car| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n|          Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n|       Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n|        Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n|         Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n|         Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+\nonly showing top 10 rows\n\n"
                }
            ], 
            "source": "sdf.show(10)"
        }, 
        {
            "source": "* ***mpg \uce7c\ub7fc\ub9cc query \ud574\uc11c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----+\n| mpg|\n+----+\n|21.0|\n|21.0|\n|22.8|\n|21.4|\n|18.7|\n+----+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "sdf.select('mpg').show(5)"
        }, 
        {
            "source": "* ***where \uc870\uac74\ub3c4 \ud55c\ubc88 \uc918 \ubcf4\uace0\uc694***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n|                car| mpg|cyl| disp| hp|drat|                wt| qsec| vs| am|gear|carb|\n+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n|         Duster 360|14.3|  8|360.0|245|3.21|              3.57|15.84|  0|  0|   3|   4|\n|          Merc 280C|17.8|  6|167.6|123|3.92|              3.44| 18.9|  1|  0|   4|   4|\n|         Merc 450SE|16.4|  8|275.8|180|3.07|              4.07| 17.4|  0|  0|   3|   3|\n|         Merc 450SL|17.3|  8|275.8|180|3.07|              3.73| 17.6|  0|  0|   3|   3|\n|        Merc 450SLC|15.2|  8|275.8|180|3.07|              3.78| 18.0|  0|  0|   3|   3|\n| Cadillac Fleetwood|10.4|  8|472.0|205|2.93|              5.25|17.98|  0|  0|   3|   4|\n|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.4239999999999995|17.82|  0|  0|   3|   4|\n|  Chrysler Imperial|14.7|  8|440.0|230|3.23|             5.345|17.42|  0|  0|   3|   4|\n|   Dodge Challenger|15.5|  8|318.0|150|2.76|              3.52|16.87|  0|  0|   3|   2|\n|        AMC Javelin|15.2|  8|304.0|150|3.15|             3.435| 17.3|  0|  0|   3|   2|\n|         Camaro Z28|13.3|  8|350.0|245|3.73|              3.84|15.41|  0|  0|   3|   4|\n|     Ford Pantera L|15.8|  8|351.0|264|4.22|              3.17| 14.5|  0|  1|   5|   4|\n|      Maserati Bora|15.0|  8|301.0|335|3.54|              3.57| 14.6|  0|  1|   5|   8|\n+-------------------+----+---+-----+---+----+------------------+-----+---+---+----+----+\n\n"
                }
            ], 
            "source": "sdf.filter(sdf['mpg'] < 18).show()"
        }, 
        {
            "source": "<a id='Aggregate_data_after_grouping_by_columns'></a>\n## 4. Aggregate data after grouping by columns\n* ***cyl \uce7c\ub7fc\uc73c\ub85c gropu by \ud574\uc11c  wt \uac12\uc744 \ud3c9\uade0\ub0c5\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+------------------+\n|cyl|           avg(wt)|\n+---+------------------+\n|  6| 3.117142857142857|\n|  8|3.9992142857142867|\n|  4| 2.285727272727273|\n+---+------------------+\n\n"
                }
            ], 
            "source": "sdf.groupby(['cyl']).agg({\"wt\": \"AVG\"}).show()"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+-----------------+\n|cyl|          sum(wt)|\n+---+-----------------+\n|  6|            21.82|\n|  8|55.98900000000001|\n|  4|           25.143|\n+---+-----------------+\n\n"
                }
            ], 
            "source": "sdf.groupby(['cyl']).agg({\"wt\": \"SUM\"}).show()"
        }, 
        {
            "source": "* ***Count \ub0b4\uc11c descending sort \ud569\ub2c8\ub2e4.***", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---------+\n|cyl|count(wt)|\n+---+---------+\n|  8|       14|\n|  4|       11|\n|  6|        7|\n+---+---------+\n\n"
                }
            ], 
            "source": "car_counts = sdf.groupby(['cyl']).agg({\"wt\": \"count\"}).sort(\"count(wt)\", ascending=False).show()"
        }, 
        {
            "source": "<a id='Operate_on_columns'></a>\n## 5. Operate on columns\nPython provides a number of functions that you can apply directly to columns for data processing. In the following example, a basic arithmetic function converts lbs to metric tons:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----------------+-----+-------+\n|              car|   wt|  wtTon|\n+-----------------+-----+-------+\n|        Mazda RX4| 2.62|  1.179|\n|    Mazda RX4 Wag|2.875|1.29375|\n|       Datsun 710| 2.32|  1.044|\n|   Hornet 4 Drive|3.215|1.44675|\n|Hornet Sportabout| 3.44|  1.548|\n|          Valiant| 3.46|  1.557|\n+-----------------+-----+-------+\nonly showing top 6 rows\n\n"
                }
            ], 
            "source": "sdf.withColumn('wtTon', sdf['wt'] * 0.45).select('car','wt','wtTon').show(6)"
        }, 
        {
            "source": "<a id='Run_SQL_queries_from_the_Spark_DataFrame'></a>\n## 6. Run SQL queries from the Spark DataFrame\nYou can register a Spark DataFrame as a temporary table and then run SQL queries over the data. The `sql` function enables an application to run SQL queries programmatically and returns the result as a DataFrame:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------+----+\n|           car|gear|\n+--------------+----+\n| Porsche 914-2|   5|\n|  Lotus Europa|   5|\n|Ford Pantera L|   5|\n|  Ferrari Dino|   5|\n| Maserati Bora|   5|\n+--------------+----+\n\n"
                }
            ], 
            "source": "sdf.registerTempTable(\"cars\")\n\nhighgearcars = sqlContext.sql(\"SELECT car, gear FROM cars WHERE gear >= 5\")\nhighgearcars.show()    "
        }, 
        {
            "source": "## That's it!\nYou successfully completed this notebook! You learned how to load a DataFrame, view and filter the data, aggregate the data, perform operations on the data in specific columns, and run SQL queries against the data. For more information about Spark, see the [Spark Quick Start Guide](http://spark.apache.org/docs/latest/quick-start.html).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Authors\n\n**Saeed Aghabozorgi**, PhD, is a Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge. He is a researcher in the data mining field and an expert in developing advanced analytic methods like machine learning and statistical modelling on large data sets.\n\n**Polong Lin** is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds an M.Sc. in Cognitive Psychology.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Copyright \u00a9 2016, 2018 Big Data University. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT License</a>.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}